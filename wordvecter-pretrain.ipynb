{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove-twitter-100d', 'jigsaw-unintended-bias-in-toxicity-classification']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import re\n",
    "import os\n",
    "import itertools\n",
    "print(os.listdir(\"../input\"))\n",
    "import emoji\n",
    "import pickle\n",
    "# Any results you write to the current directory are saved as output.\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"\")\n",
    "class process_description(object):\n",
    "    def __init__(self,vec_dim = 128):\n",
    "        self.model = None\n",
    "        self.max_len = 0\n",
    "        self.unknown_vect = None\n",
    "        self.padding_vect = np.zeros([100,])\n",
    "        self.word2vec_dict = {}\n",
    "        self.word2vec_vocab = {}\n",
    "        self.vec_dim = vec_dim\n",
    "    def process_description_file(self,file,max_len =  0.99):\n",
    "        print('\\nProcessing text data...')\n",
    "        file = file.fillna('').str.lower()\n",
    "        #file = file.progress_apply(emoji.demojize)\n",
    "        #file = file.progress_apply(lambda x:re.sub('_',' ',x))        \n",
    "        file = file.progress_apply(lambda x: re.sub('[0-9][0-9a-z]*', '<number>', x))\n",
    "        file = file.progress_apply(lambda x: re.findall(r\"\\w+|<\\w+>|[^\\w\\s:]\", x, re.UNICODE))\n",
    "        text_len = file.apply(len)\n",
    "        self.max_len = int(np.ceil(text_len.quantile(0.99)))\n",
    "        print('The length of sentences: ',self.max_len)\n",
    "        print('Data processed sucessfully.')\n",
    "        return file\n",
    "    def train_word2vec(self,file,word2vec_epochs = 10,min_count = 1,similar_words = None):\n",
    "        print('\\nTraning word2vec model...')\n",
    "        start_time = time.time()\n",
    "        self.model = Word2Vec(file, \n",
    "                              iter=word2vec_epochs, \n",
    "                              min_count=min_count , \n",
    "                              size=self.vec_dim,\n",
    "                              window=5,\n",
    "                              workers = 4)\n",
    "        self.model.save('word2vec.model')\n",
    "        self.word2vec_dict = self.model.wv\n",
    "        self.word2vec_vocab = self.model.wv.vocab\n",
    "        self.unknown_vect = np.mean(self.model.wv.syn0,axis = 0)\n",
    "        print('Training done. Time used:',time.time()-start_time)\n",
    "        if similar_words:\n",
    "            print(self.model.most_similar(str(similar_words),topn =5))\n",
    "    def padding_file(self,file):\n",
    "        file = pad_sequences(file,\n",
    "            maxlen = self.max_len,\n",
    "            dtype=object,\n",
    "            padding = 'post',\n",
    "            truncating = 'post',\n",
    "            value = 'PADDING')\n",
    "        return file\n",
    "\n",
    "    def token2vec(self,token):\n",
    "        if token is 'PADDING':\n",
    "            return self.padding\n",
    "        elif token in self.word2vec_vocab:\n",
    "            return self.word2vec_dict[token]\n",
    "        else:\n",
    "            return self.unknown_vect\n",
    "    def sent2vec(self,sentence):\n",
    "        return np.array(list(map(self.token2vec,sentence)))\n",
    "    def text_to_vect(self,file):\n",
    "        print('\\nConverting sentences to vectors')\n",
    "        sent_vector = []\n",
    "        start_time = time.time()\n",
    "        for sentence in tqdm(file):\n",
    "            sent_vector.append(self.sent2vec(sentence))\n",
    "        #sent_vector = np.array(sent_vector)\n",
    "        print('Converted sucessfully. Time used:',time.time()-start_time)\n",
    "        return np.array(sent_vector)\n",
    "#desc_parser = process_description()\n",
    "#desc_parser.process_description_file(pd.Series(['1123 is d0! 💙', '2thd the main we! 2o3 to do'] ))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GloVe files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_path = \"../input/glove-twitter-100d/glove.twitter.27B.100d.txt\" \n",
    "glove_embedding_index = {}\n",
    "f = open(glove_embedding_path)\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove_embedding_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(glove_embedding_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train and test files and pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    nrows = 10000\n",
    "else:\n",
    "    nrows = None\n",
    "    \n",
    "train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\",usecols = ['id','comment_text','target'],nrows = nrows)\n",
    "test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\",usecols = ['id','comment_text'], nrows = nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264764\n",
      "1246819\n",
      "540110\n",
      "539202\n",
      "(1883341, 3)\n",
      "\n",
      "Processing text data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "100%|██████████| 1883341/1883341 [00:11<00:00, 167730.14it/s]\n",
      "100%|██████████| 1883341/1883341 [01:03<00:00, 29584.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of sentences:  217\n",
      "Data processed sucessfully.\n"
     ]
    }
   ],
   "source": [
    "train_0 = train[train.target == 0]\n",
    "train_non_0 = train[train.target != 0]\n",
    "\n",
    "del train\n",
    "# clean duplicates\n",
    "print(len(train_0))\n",
    "train_0 = train_0.drop_duplicates(subset = ['comment_text'])\n",
    "print(len(train_0))\n",
    "\n",
    "print(len(train_non_0))\n",
    "train_non_0 = train_non_0.drop_duplicates(subset = ['comment_text','target'])\n",
    "print(len(train_non_0))\n",
    "# merge text\n",
    "total_text = pd.concat([train_0,train_non_0,test],ignore_index = True)\n",
    "print(total_text.shape)\n",
    "\n",
    "#process data\n",
    "desc_parser = process_description()\n",
    "tokens = desc_parser.process_description_file(total_text['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>59849</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>59852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>59855</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FFFFUUUUUUUUUUUUUUU</td>\n",
       "      <td>59863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text     id  target\n",
       "0  This is so cool. It's like, 'would you want yo...  59848     0.0\n",
       "1  Thank you!! This would make my life a lot less...  59849     0.0\n",
       "2  This is such an urgent design problem; kudos t...  59852     0.0\n",
       "3  Is this something I'll be able to install on m...  59855     0.0\n",
       "4                                FFFFUUUUUUUUUUUUUUU  59863     0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1883336</th>\n",
       "      <td>That's the thing...it's called a plan.  Get in...</td>\n",
       "      <td>7097315</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883337</th>\n",
       "      <td>It's not quite the way you describe it, Mike. ...</td>\n",
       "      <td>7097316</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883338</th>\n",
       "      <td>What right have you to criticise? You have no ...</td>\n",
       "      <td>7097317</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883339</th>\n",
       "      <td>My concern is that if China allies with the US...</td>\n",
       "      <td>7097318</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883340</th>\n",
       "      <td>Dear Young Philip, General Kelly can speak for...</td>\n",
       "      <td>7097319</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment_text   ...    target\n",
       "1883336  That's the thing...it's called a plan.  Get in...   ...       NaN\n",
       "1883337  It's not quite the way you describe it, Mike. ...   ...       NaN\n",
       "1883338  What right have you to criticise? You have no ...   ...       NaN\n",
       "1883339  My concern is that if China allies with the US...   ...       NaN\n",
       "1883340  Dear Young Philip, General Kelly can speak for...   ...       NaN\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_text.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118889003/118889003 [01:55<00:00, 1032428.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35879453560335206 of unique tokens were embedded.\n",
      "0.9935801631711891 of text were embeded\n",
      "0.9102825773304712 of Glove keys were usesless\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_tokens = list(itertools.chain.from_iterable(tokens))\n",
    "unique_tokens = set(total_tokens)\n",
    "len_unique_tokens = len(unique_tokens)\n",
    "len_unique_tokens = len(unique_tokens)\n",
    "len_glove = len(glove_embedding_index)\n",
    "len_tokens = len(total_tokens)\n",
    "ood = {}\n",
    "ind = {}\n",
    "within = 0\n",
    "for token in tqdm(total_tokens):\n",
    "    if token in  glove_embedding_index:\n",
    "        within += 1\n",
    "        if token in ind:\n",
    "            ind[token] += 1\n",
    "        else:\n",
    "            ind[token] = 1\n",
    "            \n",
    "    else:\n",
    "        if token in ood:\n",
    "            ood[token] += 1\n",
    "        else:\n",
    "            ood[token] =1\n",
    "len_ood = len(ood)\n",
    "len_ind = len(ind)\n",
    "useless_glove = set(glove_embedding_index.keys()).difference(set(ind.keys()))\n",
    "print(\"{} of unique tokens were embedded.\\n{} of text were embeded\\n{} of Glove keys were usesless\".format(len_ind/(len_unique_tokens), within/len_tokens, len(useless_glove)/len_glove) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\xad', 17386),\n",
       " ('murkowski', 3570),\n",
       " ('siemian', 2180),\n",
       " ('brexit', 2041),\n",
       " ('notley', 1707),\n",
       " ('tillerson', 1585),\n",
       " ('theglobeandmail', 1421),\n",
       " ('manafort', 1396),\n",
       " ('drumpf', 1288),\n",
       " ('gorsuch', 1203),\n",
       " ('alceste', 1150),\n",
       " ('magisterium', 1093),\n",
       " ('sloter', 1034),\n",
       " ('usccb', 1012),\n",
       " ('gabbard', 1002),\n",
       " ('kealoha', 961),\n",
       " ('tfsa', 959),\n",
       " ('eweb', 942),\n",
       " ('utm_term', 938),\n",
       " ('trumpster', 889),\n",
       " ('trumpcare', 869),\n",
       " ('\\u200b', 841),\n",
       " ('chaput', 782),\n",
       " ('imua', 754),\n",
       " ('klastri', 754),\n",
       " ('softwood', 743),\n",
       " ('bigly', 724),\n",
       " ('wavemaker', 701),\n",
       " ('fptp', 664),\n",
       " ('massengill', 662),\n",
       " ('shannyn', 659),\n",
       " ('punahou', 640),\n",
       " ('ibbitson', 600),\n",
       " ('trumpers', 598),\n",
       " ('anwr', 595),\n",
       " ('trumpism', 589),\n",
       " ('rhyner', 573),\n",
       " ('monsef', 569),\n",
       " ('clericalism', 568),\n",
       " ('zinke', 566),\n",
       " ('😂', 565),\n",
       " ('coghill', 562),\n",
       " ('wohlforth', 538),\n",
       " ('fluoridated', 537),\n",
       " ('gasline', 527),\n",
       " ('kellyanne', 527),\n",
       " ('hanabusa', 524),\n",
       " ('mushers', 514),\n",
       " ('pfds', 504),\n",
       " ('_r', 501)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ood = {k: v for k, v in sorted(ood.items(), key=lambda x: x[1],reverse = True)}\n",
    "list(ood.items())[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding layers weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107079\n",
      "Number of unique tokens:  107081\n"
     ]
    }
   ],
   "source": [
    "print(len(ind))\n",
    "# if the word not in the embedding, using <unk> instead. The vector of <unk> is the mean of all vectors, the <pad> keeps 0\n",
    "word_index = {'<pad>':0,'<unk>':1}\n",
    "for i,word in enumerate(ind.keys()):\n",
    "    word_index[word] = i+2\n",
    "print(\"Number of unique tokens: \",len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107082, 100)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = glove_embedding_index['i'].shape[0]\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix[1] = embedding_matrix.mean(axis = 0)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"word_index.pkl\",\"wb\")\n",
    "pickle.dump(word_index,f)\n",
    "f.close()\n",
    "\n",
    "pd.DataFrame(embedding_matrix).to_csv('embedding_weights_twitter_100.csv', index=False,header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nemoji_dict = {}\\nemojis = [\\'😂\\', \\'😁\\', \\'🔥\\', \\'🌮\\', \\'😉\\', \\'😀\\', \\'🆘\\', \\'👍\\', \\'😊\\', \\'🙄\\', \\'😄\\', \\'😃\\',\\'😆\\', \\'👎\\',\\'😜\\', \\'😡\\', \\'🤔\\',\\n\\'🌯\\', \\'😎\\', \\'💨\\', \\'🤣\\', \\'💥\\', \\'💙\\']\\nfor e in emojis:\\n    emoji_dict[e] = re.sub(\":\",\"\",re.sub(\"_\",\" \",emoji.demojize(e)))\\nemoji_dict\\n\\nfrom textblob import TextBlob\\ncorrect_word = []\\nfor word in tqdm(ood.keys()):\\n    corrected_word,coef = Word(word).spellcheck()[0]\\n    if word != corrected_word:\\n        correct_word.append([word,corrected_word,coef])   \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "emoji_dict = {}\n",
    "emojis = ['😂', '😁', '🔥', '🌮', '😉', '😀', '🆘', '👍', '😊', '🙄', '😄', '😃','😆', '👎','😜', '😡', '🤔',\n",
    "'🌯', '😎', '💨', '🤣', '💥', '💙']\n",
    "for e in emojis:\n",
    "    emoji_dict[e] = re.sub(\":\",\"\",re.sub(\"_\",\" \",emoji.demojize(e)))\n",
    "emoji_dict\n",
    "\n",
    "from textblob import TextBlob\n",
    "correct_word = []\n",
    "for word in tqdm(ood.keys()):\n",
    "    corrected_word,coef = Word(word).spellcheck()[0]\n",
    "    if word != corrected_word:\n",
    "        correct_word.append([word,corrected_word,coef])   \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
